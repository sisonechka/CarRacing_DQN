{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"BReeWH_OdUkb"},"outputs":[],"source":["import gymnasium\n","from utils import*\n","import torch\n","import torch.nn as nn\n","import numpy as np\n","from collections import deque\n","import random\n","from tqdm import tqdm\n","import gc\n","import matplotlib.pyplot as plt\n","\n","def simulate(agent=None, env=None, epsilon=0, memory=None, render=False):\n","    agent.eval()  # Set the agent to evaluation mode.\n","    env.reset()  # Reset the environment to start a new episode.\n","    \n","    if render:\n","        env.env.render()  # Render the environment if specified.\n","    \n","    # Skip initial episodes to focus on the main part of the simulation.\n","    state, rew, done, info = env.skip_episodes(70, [0, 0.1, 0])\n","    ep_len = 0\n","    \n","    while not done:\n","        # Decide whether to take a random action or use the agent's policy.\n","        sample = torch.bernoulli(torch.tensor(epsilon).float())\n","        if sample == 1:\n","            A = torch.randint(0, 3, (1,))\n","        else:\n","            A = agent.get_action(state)\n","        \n","        # Perform the selected action and progress to the next time step.\n","        next_state, rew, done, info = env.step(agent.convert_action(A, state))\n","        \n","        if done:  # If the episode is finished, exit the loop.\n","            break\n","\n","        # Store the experience in memory for later learning.\n","        if memory is not None:\n","            memory.collect([state, A, rew, next_state])\n","        state = next_state\n","\n","        ep_len = env.ep_len\n","        # Stop the episode if it exceeds the maximum length.\n","        if ep_len > 2000:\n","            break\n","\n","    # Adjust the final reward to account for any penalties or adjustments.\n","    score = env.real_rew\n","    if render:\n","        print(\"score\", score, \"ep_len\", ep_len)\n","        env.env.close()\n","  \n","    return score, ep_len\n","\n","\n","def test_model(agent, env, episodes=1):\n","    rewards = []\n","    ep_lens = []\n","    \n","    # Simulate multiple episodes to test the agent's performance.\n","    for i in range(episodes):\n","        rew, ep_len = simulate(agent, env)\n","        rewards.append(rew)\n","        ep_lens.append(ep_len)\n","        print(\"Test \" + str(i+1) + \"/\" + str(episodes) + \": reward =\", rew, \" episode len =\", ep_len)\n","    \n","    # Print the average reward and episode length across all test episodes.\n","    print(\"\\nAverage Reward =\", sum(rewards) / len(rewards), \"Average Ep_len =\", sum(ep_lens) / len(ep_lens), \"\\n\")\n","    return rewards, ep_lens\n","\n","\n","class ExperienceReplay(object):\n","    def __init__(self, length):\n","        # Initialize the experience replay buffer with a fixed maximum size.\n","        self.experience_replay = deque(maxlen=length)\n","\n","    def collect(self, experience):\n","        # Add a new experience to the replay buffer.\n","        self.experience_replay.append(experience)\n","        return\n","  \n","    def sample_from_experience(self, sample_size):\n","        # Randomly sample a batch of experiences from the replay buffer.\n","        sample_size = min(sample_size, len(self.experience_replay))\n","        sample = random.sample(self.experience_replay, sample_size)\n","        \n","        # Extract states, actions, rewards, and next states from the sampled experiences.\n","        state = torch.tensor([episode[0] for episode in sample]).float()\n","        action = torch.tensor([episode[1] for episode in sample]).float()\n","        reward = torch.tensor([episode[2] for episode in sample]).float()\n","        next_state = torch.tensor([episode[3] for episode in sample]).float()\n","\n","        return state, action, reward, next_state\n","\n","\n","class DQN_Network_Simple(nn.Module):\n","    def __init__(self, gamma=None):\n","        super().__init__()\n","        \n","        # Define the layers of the neural network.\n","        self.LeakyReLU = nn.LeakyReLU()\n","        self.conv1 = nn.Conv2d(1, 4, kernel_size=7, stride=4, padding=0)\n","        self.conv2 = nn.Conv2d(4, 8, kernel_size=3, stride=1, padding=2)\n","        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n","        self.fc1 = nn.Linear(289, 100)\n","        self.fc2 = nn.Linear(100, 3)\n","        self.batchnormCNN1 = nn.BatchNorm2d(num_features=4)\n","        self.batchnormCNN2 = nn.BatchNorm2d(num_features=8)\n","        self.flatten = nn.Flatten()\n","        self.gamma = gamma\n","    \n","    def forward(self, x):\n","        # Reformat the input image to the required shape for the network.\n","        x = torch.from_numpy(np.ascontiguousarray(x)).float()\n","        if x.dim() == 2:\n","            x = torch.unsqueeze(x, dim=0)\n","            x = torch.unsqueeze(x, dim=0)\n","        elif x.dim() == 3:\n","            x = torch.unsqueeze(x, dim=1)\n","        \n","        # Process the speed information from the image.\n","        subimage = (x[:, :, 84:96, 13:14] - 0.495) * 10\n","        speed = torch.sum(subimage, dim=(2, 3))\n","        x = x[:, :, :84, :]\n","        \n","        # Pass the image through the convolutional and pooling layers.\n","        x = self.LeakyReLU(self.conv1(x))\n","        x = self.pool(x)\n","        x = self.LeakyReLU(self.conv2(x))\n","        x = self.pool(x)\n","        x = self.flatten(x)\n","        \n","        # Concatenate the flattened image features with the speed information.\n","        x = torch.cat((x, speed), dim=1)\n","        x = self.LeakyReLU(self.fc1(x))\n","        x = self.fc2(x)\n","        \n","        # Return the output of the network.\n","        return x\n","    \n","    def get_action(self, state):\n","        # Pass the state through the network to get Q-values and select the best action.\n","        qvals = self.forward(state)\n","        return torch.argmax(qvals, 1)\n","  \n","    def convert_action(self, action, state):\n","        # Convert the discrete action to the corresponding control inputs for the environment.\n","        subimage = (state[84:96, 13:14] - 0.495) * 10\n","        speed = np.sum(subimage).item()\n","        \n","        # Determine the acceleration based on the speed.\n","        if speed > 3.5:\n","            accel = 0\n","        elif speed > 2.5:\n","            accel = 0\n","        else:\n","            accel = 0.1\n","        \n","        action = action.item()\n","        if action == 0:\n","            return [-0.3, accel, 0]  # Turn left with specified acceleration.\n","        elif action == 1:\n","            return [0, accel, 0]  # Go straight with specified acceleration.\n","        elif action == 2:\n","            return [0.3, accel, 0]  # Turn right with specified acceleration.\n"]},{"cell_type":"markdown","metadata":{"id":"arykaWSyIXJ0"},"source":["### Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GkB_3IPIEK5l"},"outputs":[],"source":["def load_memory(new, epsilon, exp_replay_size, initial_size=None):\n","    if initial_size is None:\n","        initial_size = exp_replay_size\n","\n","    # Create the environment and the model, select render mode (human\\rgb_array)\n","    env = modified_env(gymnasium.make(\"CarRacing-v2\", render_mode=\"rgb_array\").unwrapped)\n","    agent = DQN_Network_Simple()\n","    \n","    # Load a pre-trained model if it's not a new run\n","    if not new:\n","        agent.load_state_dict(torch.load(\"car-racing-dqn.pth\"))\n","    \n","    # Initialize the experience replay buffer\n","    memory = ExperienceReplay(exp_replay_size)\n","\n","    # Populate the experience replay buffer\n","    for i in range(exp_replay_size):\n","        state = env.reset()  # Reset the environment to the initial state\n","        simulate(agent, env, epsilon=epsilon, memory=memory)  # Simulate the environment and collect experiences\n","        if len(memory.experience_replay) >= initial_size:  # Stop if the buffer is filled to the initial size\n","            break\n","        print(len(memory.experience_replay))  # Print the current size of the replay buffer\n","\n","    return memory  # Return the populated memory buffer\n","\n","\n","def plot_results(reward_hist, ep_len_hist):\n","    plt.figure(figsize=(12, 5))\n","    \n","    # Plot the reward progression over episodes\n","    plt.subplot(1, 2, 1)\n","    plt.plot(reward_hist, label='Reward')\n","    plt.xlabel('Episodes')\n","    plt.ylabel('Reward')\n","    plt.title('Reward Progression')\n","    plt.legend()\n","    \n","    # Plot the episode length progression over episodes\n","    plt.subplot(1, 2, 2)\n","    plt.plot(ep_len_hist, label='Episode Length', color='orange')\n","    plt.xlabel('Episodes')\n","    plt.ylabel('Length')\n","    plt.title('Episode Length Progression')\n","    plt.legend()\n","    \n","    plt.tight_layout()\n","    plt.show()  # Display the plots\n","\n","\n","def update(agent, optimizer, loss_func, target_agent, memory, batch_size):\n","    agent.train()  # Set the agent to training mode\n","    target_agent.eval()  # Set the target agent to evaluation mode\n","    \n","    # Sample a batch of experiences from memory\n","    state, action, reward, next_state = memory.sample_from_experience(batch_size)\n","    \n","    # Compute the Q-values for the current states\n","    Qvals = agent(state)\n","    curr_Qval = Qvals[torch.arange(Qvals.size(0)), action.long()]\n","  \n","    # Compute the target Q-values using the target agent\n","    with torch.no_grad():\n","        next_Qval, indices = torch.max(target_agent(next_state), dim=1)\n","\n","    # Compute the loss and update the agent\n","    loss = loss_func(reward + agent.gamma * next_Qval, curr_Qval)\n","    loss.backward(retain_graph=False)\n","    optimizer.step()\n","    optimizer.zero_grad()  # Reset the gradients\n","\n","\n","def train(new, num_ep, lr_start, epsilon_start, gamma, memory):\n","    # Initialize the agent with the given discount factor\n","    agent = DQN_Network_Simple(gamma=gamma)\n"," \n","    # Initialize performance tracking lists\n","    if new:\n","        reward_hist = []\n","        ep_len_hist = []\n","        lr_hist = []\n","        epsilon_hist = []\n","    # Load a previously trained model if it's not a new run\n","    else:\n","        agent.load_state_dict(torch.load(\"car-racing-dqn.pth\"))\n","\n","    # Initialize the target agent and other components\n","    target_agent = DQN_Network_Simple(agent.gamma)  # Target agent for more stable updates\n","    target_agent.load_state_dict(agent.state_dict())\n","    env = modified_env(gymnasium.make(\"CarRacing-v2\", render_mode=\"rgb_array\").unwrapped)\n","    optimizer = torch.optim.SGD(agent.parameters(), lr_start)\n","    MSELoss = torch.nn.MSELoss()\n","\n","    # Main training loop\n","    for ep_num in tqdm(range(0, num_ep)):\n","        # Update learning rate and epsilon\n","        lr = lr_start * (0.99042 ** ep_num)\n","        epsilon = epsilon_start * (0.99424 ** ep_num)\n","\n","        for param_group in optimizer.param_groups:  # Update the learning rate\n","            param_group['lr'] = lr\n","\n","        state, done, losses, ep_len, reward = env.reset(), False, 0, 0, 0\n","        reward, ep_len = simulate(agent, env, epsilon=epsilon, memory=memory)  # Run a simulation and collect the reward and episode length\n","  \n","        # Update the agent multiple times per episode\n","        for i in range(0, 30):\n","            update(agent, optimizer, MSELoss, target_agent, memory, batch_size=32)\n","        target_agent.load_state_dict(agent.state_dict())  # Sync the target agent with the main agent\n","        gc.collect(generation=2)  # Perform garbage collection to free memory\n","\n","        if ep_num % 3 == 0:\n","            # Test the model and track performance\n","            reward, ep_len = test_model(agent=agent, env=env, episodes=1)\n","            print(\"Settings: lr =\", lr, \"epsilon =\", epsilon)\n","            print(\"Test Result: reward =\", reward[0], \"episode length =\", ep_len[0])\n","            reward_hist.append(reward[0])\n","            ep_len_hist.append(ep_len[0])\n","            lr_hist.append(lr)\n","            epsilon_hist.append(epsilon)\n","    \n","        if ep_num % 30 == 0:\n","            # Save the model every 30 episodes\n","            torch.save(agent.state_dict(), \"car-racing-dqn.pth\")\n","    \n","    return reward_hist, ep_len_hist  # Return the history of rewards and episode lengths\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":2240911,"status":"error","timestamp":1655833436162,"user":{"displayName":"tigerfan707","userId":"17162744304693628162"},"user_tz":240},"id":"_ijJZqDaVqEr","outputId":"27f66d78-612e-458d-d452-5760f800c96b"},"outputs":[],"source":["memory = load_memory(new=True,epsilon=1,exp_replay_size=500)\n","reward_hist, ep_len_hist = train(new=True,num_ep = 10,lr_start=0.008,epsilon_start=0.8,gamma=0.94,memory=memory)\n","plot_results(reward_hist, ep_len_hist)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":546},"executionInfo":{"elapsed":30514,"status":"ok","timestamp":1655833543630,"user":{"displayName":"tigerfan707","userId":"17162744304693628162"},"user_tz":240},"id":"7Go4nqY0dtXJ","outputId":"f0e2d8ab-529b-4127-9e01-aa0f5dc98e69"},"outputs":[],"source":["plot_results(reward_hist, ep_len_hist)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["env = modified_env(gymnasium.make(\"CarRacing-v2\", render_mode=\"rgb_array\").unwrapped)\n","agent = DQN_Network_Simple()\n","agent.load_state_dict(torch.load(\"car-racing-dqn.pth\"))\n","\n","simulate(agent=agent,env=env,render=True)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyM8smKj2M5EL/XSETfG8u3b","collapsed_sections":[],"machine_shape":"hm","name":"DDQN.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":0}
